Abstract

A user interface is designed to control the computer pointer using hand de- tection and classification of its gesture. A dataset with 6720 image samples is collected, including four categories: fist, palm, left, and right. The images are captured from 15 individuals in simple backgrounds and different per- spectives and light conditions. A CNN network is trained on the dataset to predict a label for each captured image and measure its similarity with other samples. Finally, commands are defined so that the user can click, right-click and move the cursor based on their hand gestures. The algorithm reaches 91.88% accuracy and can be used in different simple backgrounds.

Keywords: Hand Gesture Recognition, Dataset, Convolutional Neural Network, Human-Computer Interaction, Classification
